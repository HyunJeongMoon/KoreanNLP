{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PEM118EXHYoixJUJ7OIDCgO8gZwB3Mbw","timestamp":1710673904186}],"authorship_tag":"ABX9TyNtI9Tnm2xsP8TVEIjsmap1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# 코랩에서 실행시 먼저 설치해야 한다\n","!pip install JPype1\n","!pip install konlpy\n","!pip install pyMySQL\n","!pip install -U finance-datareader"],"metadata":{"id":"WBN80c1MEt0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Okh7SBcWBuHT"},"outputs":[],"source":["# import packages\n","import os\n","import numpy as np\n","import pandas as pd\n","import FinanceDataReader as fdr\n","import sqlite3\n","from sqlalchemy import create_engine\n","import konlpy\n","from konlpy.tag import Okt\n","import pymysql\n","from os import replace\n","import requests\n","from bs4 import BeautifulSoup\n","from tqdm.notebook import tqdm\n","from datetime import datetime\n","from sqlalchemy import create_engine\n","import ast\n","from os import replace\n","import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import TextVectorization\n","import re\n","import string\n","# 리스트 flatten을 위한 itertools 패키지 import\n","import itertools\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","source":["#* 1. 데이터 수집\n","- 1.1 주가 OHLCV 데이터\n","  - 샘플 종목: 005930\n","- 1.2 뉴스데이터"],"metadata":{"id":"VEUiJFWJFjC9"}},{"cell_type":"code","source":["# 주가 데이터 확인\n","# DB 연결 준비\n","conn = sqlite3.connect('/content/nlp_sample.db')\n","# 종목 지정:  stock_code 005930\n","scode = '005930'\n","# DB에서 종목 데이터  추출위한 sql 준비\n","sql = f\"SELECT * FROM A{scode} \"\n","# DB 검색결과를 dataframe에 저장\n","df_stock_price = pd.read_sql_query(sql, conn)\n","# DB close\n","conn.close()\n","\n","# 실행확인을 위한 화면 출력\n","print(df_stock_price.head())"],"metadata":{"id":"KfWy-rbzE4ao"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#* 뉴스 데이터\n","- 네이버 증권, 실시간 속보\n","- beautifulsoup 사용"],"metadata":{"id":"ApuPsDZ9Gde-"}},{"cell_type":"code","source":["# 뉴스 데이터 수집부터\n","# 크롤링 할 웹주소 URL\n","news_url= f\"https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258&date=20230823&page=13\"\n","# 웹 request 위한 html 형식 설정\n","html = requests.get(news_url)\n","# BeautifulSoup 으로 해당 웹페이지 읽어오기\n","soup = BeautifulSoup(html.text, \"lxml\")"],"metadata":{"id":"nXjux35bGCRI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행확인을 위한 화면 출력\n","print(soup)"],"metadata":{"id":"M4oEKwkgxL1L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 가져올 뉴스가 있는 부분 확인\n","# <dd class='articleSummary'> 태그 목록 만들기\n","ddsum_list = soup.find_all('dd', attrs={\"class\": \"articleSummary\"})"],"metadata":{"id":"1R0zjDvrHmvQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행확인을 위한 화면 출력\n","ddsum_list[0]"],"metadata":{"id":"WWJK8peYKJgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 뉴스만 읽어오기\n","# 현재 페이지 전체 뉴스 저장을 위한 news_df 생성\n","news_df = pd.DataFrame()\n","# 뉴스 있는 동안 반복\n","if len(ddsum_list)>0:\n","  # <dd> 태그 목록에서 하나씩 꺼내기\n","  for ddsum in ddsum_list:\n","    # <dd> 와 </dd> 사이의 텍스트를 추출, 다른 태그 있을시 '^'구분해서 추출, 앞뒤 공백제거\n","    data_string = ddsum.getText('^').strip()\n","    # 추출한 문자열을 '^' 경계로 분리\n","    string_list = data_string.split('^')\n","    # 2줄 요약 news_summary 문자열 추출\n","    news_summary  = string_list[0].strip()\n","    # 신문사 press_name 문자열 추출\n","    press_name    = string_list[1].strip()\n","    # 배포날짜 release_date 문자열 추출\n","    release_date  = string_list[5].strip()\n","    # 추출한 위 3 항목을 DataFrame으로 저장\n","    temp_df = pd.DataFrame(data = [[news_summary,press_name,release_date]], columns=['summary','press','rdate'])\n","    # 현재 페이지 전체 뉴스를 위한 news_df에 추가\n","    news_df = pd.concat([news_df, temp_df], ignore_index=True)"],"metadata":{"id":"EA6qDqnWIqvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행 확인을 위한 화면 출력\n","news_df"],"metadata":{"id":"0WLtvDYPT0Q6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 웹크롤링으로 가져온 뉴스를 DB에 저장하기 & 꺼내오기\n","#==================================================================\n","# 뉴스를 DB에 저장하기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 데이터프레임을 SQLite 테이블로 저장\n","news_df.to_sql('news_4train', conn, if_exists='replace', index=False)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#==================================================================\n","\n","#==================================================================\n","# 뉴스를 DB에서 꺼내오기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 읽어오는 SQL\n","query = \"SELECT * FROM news_4train\"\n","# sql 실행 결과 데이터프레임으로 받기\n","df_from_db = pd.read_sql_query(query, conn)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#=================================================================="],"metadata":{"id":"K997mYqzUrJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행 확인 위한 화면 출력\n","print(\"From DB: \", df_from_db.head())"],"metadata":{"id":"qgpE54u2W_P0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. 학습 모델을 위한 데이터 가공\n","- 1) 주가 데이터 레이블링\n","- 2) 뉴스 자연어 처리"],"metadata":{"id":"BZAk7erkYWQW"}},{"cell_type":"code","source":["# 주가 데이터 레이블링\n","#==================================================================\n","# DB에서 주가 데이터 읽어 오기\n","# DB 연결 준비\n","conn = sqlite3.connect('/content/nlp_sample.db')\n","# 종목 지정:  stock_code 041190\n","scode = '005930'\n","# DB에서 종목 데이터  추출위한 sql 준비\n","sql = f\"SELECT * FROM A{scode} \"\n","# DB 검색결과를 dataframe에 저장\n","df_stock_price = pd.read_sql_query(sql, conn)\n","# DB close\n","conn.close()\n","#=================================================================="],"metadata":{"id":"-ibSBU4yYVpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행확인을 위한 화면 출력\n","print(df_stock_price.head())"],"metadata":{"id":"mAM8_X65aF7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#==================================================================\n","# 레이블링한 테이블 만들기 위한 df 준비\n","df_bun_labeled= df_stock_price.copy()\n","# 분당 변동률도 구해서 넣고\n","df_bun_labeled['bun_rate'] = ((df_stock_price['close']-df_stock_price['open'])/df_stock_price['open'])*100\n","# 레이블도 넣기: Up,Hold,Down\n","df_bun_labeled.loc[df_bun_labeled['bun_rate']>0,'bun_label'] = 'U'\n","df_bun_labeled.loc[df_bun_labeled['bun_rate']==0,'bun_label'] = 'H'\n","df_bun_labeled.loc[df_bun_labeled['bun_rate']<0,'bun_label'] = 'D'\n","# DB저장 위한 컬럼 정리\n","df_bun_labeled = df_bun_labeled[['date','bun_rate','bun_label']]\n","#=================================================================="],"metadata":{"id":"n25EF2W8aKpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행 확인을 위한 화면 출력\n","print(df_bun_labeled.head())"],"metadata":{"id":"I-xwdQqybhtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#==================================================================\n","# 레이블링한 결과 DB에 저장하기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 데이터프레임을 SQLite 테이블로 저장\n","df_bun_labeled.to_sql('A005930_bun_label', conn, if_exists='replace', index=False)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#==================================================================\n","\n","#==================================================================\n","# 레이블링한 결과 DB에서 꺼내오기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 읽어오는 SQL\n","query = \"SELECT * FROM A005930_bun_label\"\n","# sql 실행 결과 데이터프레임으로 받기\n","df_from_db = pd.read_sql_query(query, conn)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#=================================================================="],"metadata":{"id":"8EPuWHgXbrTk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행 확인을 위한 화면 출력\n","print(df_from_db.head())"],"metadata":{"id":"Ds34JElacgZE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.2 뉴스 자연어 처리\n","  - 2.2.1 konlpy 커스터마이징\n","  - 2.2.2 뉴스 토큰화\n","  - 2.2.3 뉴스 단어사전 만들기\n","  - 2.2.4 뉴스 벡터화\n"],"metadata":{"id":"ChstMJpPdRkg"}},{"cell_type":"code","source":["##########################################################\n","# konlpy 커스터마이징 테스트 코드\n","# 삼성전자 -> '삼성', '전자'(X), '삼성전자'(O) 확인\n","#--------------------------------------------------------\n","okt = Okt()\n","\n","print(okt.pos(\"삼성전자 005930 새로닉스 두산에너빌리티 \"))\n","print(okt.pos(\"에코프로 \"))\n","print(okt.pos(\"제주반도체 현대힘스 패스트파이브\"))\n","print(okt.pos(\"아이오아이 르세라핌\"))\n","#########################################################"],"metadata":{"id":"fpRPC7nieV39"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=====================================================================\n","# --------------------------------------------------------------------\n","# Konlpy에 한국거래소 상장종목 전체 종목명과 약명을 새로운 단어로 추가하기\n","# --------------------------------------------------------------------\n","# konlpy 디렉토리로 이동\n","os.chdir('/usr/local/lib/python3.10/dist-packages/konlpy/java')\n","# 현재 디렉토리 확인\n","os.getcwd()\n","# 압축 풀 임시 디렉토리 /aaa 만들기\n","os.makedirs('./aaa')\n","# 임시디렉토리로 이동\n","os.chdir('./aaa')\n","# 압축 풀기\n","!jar xvf ../open-korean-text-2.1.0.jar\n","# FinanceDataReader로 한국거래소 상장종목 전체 종목을 가져오기\n","df_krx = fdr.StockListing('KRX')\n","# 종목명만 추출하기\n","name_list = df_krx['Name']\n","# 새로운 단어목록 생성을 위한 data 변수 선언\n","data = ''\n","# 종목명을 한줄씩(\\n)씩 넣어서 목록 만들기\n","for one_name in name_list:\n","  data += one_name+'\\n'\n","# 종목명 약명 새로운 단어로 넣기\n","# DB 연결 준비: 종목명 약어 읽어오기\n","conn = pymysql.connect(host= '34.64.198.150', port = 3306, user=\"big17\", password=\"big1743211\", db=\"team1_project\", charset = 'utf8')\n","# DB에서 종목명 약어 추출위한 sql 준비\n","sql = f\"SELECT x.kor_name,x.kor_abb FROM team1_project.stock_name_abb x\"\n","# DB 검색결과를 dataframe에 저장\n","name_abb_df = pd.read_sql_query(sql, conn)\n","# 실행확인을 위한 화면 출력\n","print(name_abb_df.head(3))\n","# DB close\n","conn.close()\n","# 종목명 추가하기\n","name_list = name_abb_df['kor_name']\n","# 종목명을 한줄씩(\\n)씩 넣어서 목록 만들기\n","for one_name in name_list:\n","  data += one_name+'\\n'\n","# 종목명 약명 추가하기\n","name_list = name_abb_df['kor_abb']\n","# 종목명 약명을 한줄씩(\\n)씩 넣어서 목록 만들기\n","for one_name in name_list:\n","  data += one_name+'\\n'\n","# konlpy의 company_names.txt 사전으로 저장\n","with open(\"/usr/local/lib/python3.10/dist-packages/konlpy/java/aaa/org/openkoreantext/processor/util/noun/company_names.txt\", 'w') as f:\n","    f.write(data)\n","# 임시 작업 디렉토리로 이동 확인\n","os.chdir('/usr/local/lib/python3.10/dist-packages/konlpy/java/aaa')\n","# 다시 압축\n","!jar cvf ../open-korean-text-2.1.0.jar *\n","#------------------------------------------------------------------\n","# 코랩 실행시 Restart session 실행\n","# import도 다시 실행\n","#=================================================================="],"metadata":{"id":"Ct72j6nbdQOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import FinanceDataReader as fdr\n","import sqlite3\n","from sqlalchemy import create_engine\n","import konlpy\n","from konlpy.tag import Okt\n","import pymysql\n","from os import replace\n","import requests\n","from bs4 import BeautifulSoup\n","from tqdm.notebook import tqdm\n","from datetime import datetime\n","from sqlalchemy import create_engine\n","import ast\n","from os import replace\n","import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import TextVectorization\n","import re\n","import string\n","# 리스트 flatten을 위한 itertools 패키지 import\n","import itertools\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"MpRKB8Y-DkcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##########################################################\n","# konlpy 커스터마이징 테스트 코드\n","# 삼성전자 -> '삼성', '전자'(X), '삼성전자'(O) 확인\n","#--------------------------------------------------------\n","okt = Okt()\n","\n","print(okt.pos(\"삼성전자 005930 새로닉스 두산에너빌리티\"))\n","print(okt.pos(\"에코프로 \"))\n","print(okt.pos(\"제주반도체 현대힘스 패스트파이브\"))\n","print(okt.pos(\"아이오아이 르세라핌\"))\n","#########################################################"],"metadata":{"id":"-9O1rv_ve3j9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#====================================================================\n","# 뉴스 토큰화 하기\n","#-------------------------------------------------------------------\n","# 뉴스를 DB에서 꺼내오기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 읽어오는 SQL\n","query = \"SELECT * FROM news_4train\"\n","# sql 실행 결과 데이터프레임으로 받기\n","df_news_4train = pd.read_sql_query(query, conn)\n","# 데이터베이스 연결 종료\n","conn.close()"],"metadata":{"id":"Wo9VMruEfLsd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행확인을 위한 화면 출력\n","print(df_news_4train.head(3))"],"metadata":{"id":"lGFGoa7ngL2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------------------------\n","# 토큰화를 위한 konlpy의 Okt 객체 생성\n","okt=Okt()\n","# okt로 tokenize해서 형태소pos 추출하기 (tokenize 전체는 okt.morphs, 명사는 okt.nouns)\n","df_news_4train['morphs_tokenized_summary'] = df_news_4train['summary'].apply(okt.morphs)\n","# DB 저장을 위해 문자열로 변환\n","df_news_4train['morphs_tokenized_summary'] = df_news_4train['morphs_tokenized_summary'].astype(str)\n","# okt로 tokenize해서 형태소pos 추출하기 (tokenize 전체는 okt.morphs, 명사는 okt.nouns)\n","df_news_4train['pos_tokenized_summary'] = df_news_4train['summary'].apply(okt.pos)\n","# DB 저장을 위해 문자열로 변환\n","df_news_4train['pos_tokenized_summary'] = df_news_4train['pos_tokenized_summary'].astype(str)\n","# okt로 tokenize해서 형태소pos 추출하기 (tokenize 전체는 okt.morphs, 명사는 okt.nouns)\n","df_news_4train['nouns_tokenized_summary'] = df_news_4train['summary'].apply(okt.nouns)\n","# DB 저장을 위해 문자열로 변환\n","df_news_4train['nouns_tokenized_summary'] = df_news_4train['nouns_tokenized_summary'].astype(str)\n","# pos tokenized 에서 Modifier,Josa,Suffix,Punctuation,Foreigh 빼기, 1 자리 단어 빼기\n","df_news_4train['tokenized_summary'] = \"\"\n","# 모든 뉴스처리\n","for i in range(len(df_news_4train)):\n","  # 토큰화된 뉴스 임시 저장\n","  new_str = []\n","  # 형태소 분리된 내용을 튜플로 꺼내기\n","  for (x,y) in ast.literal_eval(df_news_4train['pos_tokenized_summary'][i]):\n","    #관형사, 조사, 접두어, 문장부호, 한자등 제외\n","    if (y!='Modifier')&(y!='Josa')&(y!='Suffix')&(y!='Punctuation')&(y!='Foreign'):\n","      # 1자리 단어도 제외\n","      if(len(str(x))>1):\n","        # 남은 것 모두 붙이기\n","        new_str.append(x)\n","  #토큰화 결과 컬럼에 넣기\n","  df_news_4train['tokenized_summary'][i]=str(new_str)\n","#-------------------------------------------------------------------"],"metadata":{"id":"c3-7O-K0fLqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행확인을 위한 화면 출력\n","df_news_4train.head(3)"],"metadata":{"id":"YFPJE-n4D-c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#===================================================================\n","# 토큰화한 결과 DB 저장하기\n","#DB 저장을 위해서 필요한 컬럼 정리\n","df_news_4train = df_news_4train[['summary','press','rdate','tokenized_summary','pos_tokenized_summary']]\n","# 실행 확인을 위한 화면 출력\n","#print(result_df.head(3))\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 데이터프레임을 SQLite 테이블로 저장\n","df_news_4train.to_sql('news_4train', conn, if_exists='replace', index=False)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#===================================================================\n","\n","#===================================================================\n","# 뉴스를 DB에서 꺼내오기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 읽어오는 SQL\n","query = \"SELECT * FROM news_4train\"\n","# sql 실행 결과 데이터프레임으로 받기\n","df_news_from_db = pd.read_sql_query(query, conn)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#==================================================================="],"metadata":{"id":"BvwHLtu7iu8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행확인을 위한 화면 출력\n","print(df_news_from_db.head(3))"],"metadata":{"id":"2Yg3FephfLnm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#===================================================================\n","# 단어사전 생성: 2만단어 단어사전 만들기: TextVectorization 커스터마이즈\n","#-------------------------------------------------------------------\n","# custom_표준화 함수\n","def custom_standardization_fn(string_tensor):\n","    # 한글의 경우 소문자 처리 안해도 됨\n","    lowercase_string = string_tensor\n","    # 문장부호중에 '%','.' 는 남기기\n","    return tf.strings.regex_replace(\n","        lowercase_string, f\"[{re.escape(string.punctuation.replace('%','').replace('.',''))}]\", \"\")\n","\n","# custom_토큰화 함수\n","def custom_split_fn(string_tensor):\n","    # 한글의 경우 형태소 분석 처리 해야 한다: 우린 이미 okt로 했음\n","    return tf.strings.split(string_tensor)\n","\n","# custom 표준화, 토큰화 함수 지정해서 TextVectoriztion 생성하기\n","text_vectorization = TextVectorization(\n","    #--- max_tokens 지정--\n","    max_tokens=20000\n","    #---------------------\n","    # 정수 인덱스로 출력\n","  ,  output_mode=\"int\"\n","    # 표준화 함수를 custom_표준화 함수로 지정\n","  ,  standardize=custom_standardization_fn\n","    # 토큰화 함수를 custom_토큰화 함수로 지정\n","  ,  split=custom_split_fn\n",")\n"],"metadata":{"id":"YeSMEFZCfLlN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 사전만들기 위해 토큰화된 text를 넣을 dataset 리스트 생성\n","dataset = []\n","# 전체 data 반복\n","for i in range (len(df_news_from_db)):\n","  # 토큰화된 text 를 하나의 리스트로 만든다\n","  dataset.append(eval(df_news_from_db['tokenized_summary'][i]))\n","#  중첩된 리스트를 flatten 시킨다\n","dataset = list(itertools.chain(*dataset))\n","\n","# ============================================\n","# .adapt()로 단어 사전 만들기\n","text_vectorization.adapt(dataset)\n","# ============================================\n","\n","# 어휘사전 확인 .get_vocabulary() 함수\n","len(text_vectorization.get_vocabulary())"],"metadata":{"id":"8QbGVPzHfLil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================================\n","# 단어사전 pickle로 저장\n","# --- 저장 : start ---\n","# Vector for word \"this\"\n","print (text_vectorization(\"삼성전자\"))\n","print (text_vectorization(\"LG\"))\n","print (text_vectorization(\"미국\"))\n","\n","# Pickle the config and weights\n","pickle.dump({'config': text_vectorization.get_config(),\n","             'weights': text_vectorization.get_weights()}\n","            , open(\"tv_2man.pkl\", \"wb\"))\n","\n","# Later you can unpickle and use\n","# `config` to create object and\n","# `weights` to load the trained weights.\n","# --- 저장 : end ---\n","# ========================================================="],"metadata":{"id":"_Ik8k0GPmTq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================================================\n","# 단어사전 불러오기\n","# --- 불러오기 load : start ---\n","from_disk = pickle.load(open(\"tv_2man.pkl\", \"rb\"))\n","\n","new_vectorizer = TextVectorization(\n","    # 정수 인덱스로 출력\n","    output_mode=\"int\",\n","    # 표준화 함수를 custom_표준화 함수로 지정\n","    standardize=custom_standardization_fn,\n","    # 토큰화 함수를 custom_토큰화 함수로 지정\n","    split=custom_split_fn,\n","  )\n","new_vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n","new_vectorizer.set_weights(from_disk['weights'])\n","\n","# Lets see the Vector for word \"this\"\n","print (text_vectorization(\"삼성전자\"))\n","print (text_vectorization(\"LG\"))\n","print (text_vectorization(\"미국\"))\n","# --- 불러오기 load : end ---\n","# ============================================================="],"metadata":{"id":"PrwIq7GCoisP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 벡터화 전\n","df_news_from_db.head()"],"metadata":{"id":"n1iiQqeApsvA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =============================================================\n","# 뉴스 벡터화 하기\n","# 토큰화된 것을 사전이용해서 벡터화 하고 텐서안의 인덱스 리스트를 꺼내서 따로 저장\n","for i in range(len(df_news_from_db)):\n","  df_news_from_db.at[i,'20000_encoded_text_code']= str(new_vectorizer(df_news_from_db.at[i,'tokenized_summary']).numpy()).replace('\\n', '')\n","# ============================================================="],"metadata":{"id":"0pTQVEaAoipv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 벡터화 후\n","df_news_from_db.head()"],"metadata":{"id":"QzEiGO6ToilP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#===================================================================\n","# 벡터화 결과 DB 저장하기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 데이터프레임을 SQLite 테이블로 저장\n","df_news_from_db.to_sql('news_4train', conn, if_exists='replace', index=False)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#===================================================================\n","#===================================================================\n","# 뉴스를 DB에서 꺼내오기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 읽어오는 SQL\n","query = \"SELECT * FROM news_4train\"\n","# sql 실행 결과 데이터프레임으로 받기\n","df_news_from_db = pd.read_sql_query(query, conn)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#==================================================================="],"metadata":{"id":"XVrWWconq8gC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_news_from_db.head(10)"],"metadata":{"id":"kvHraF-Rq8bw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##* 3. 데이터셋 만들기\n","* 뉴스를 분단위로 flatten 하기\n","* 뉴스 + 주가 데이터 레이블 연결\n","* - 종목 분봉 데이터를 뉴스 속보 기간에 맞춰서 자르기 : 20230823\n","* 양쪽 테이블에 join을 위한 공통 컬럼 만들기\n","- left join으로 레이블 넣기"],"metadata":{"id":"LXGZA3KuuZ8Z"}},{"cell_type":"code","source":["#---------------------------------------------------------------\n","# 뉴스 분단위 Flatten 하기\n","# 작업할 복사본 만들기\n","df_work = df_news_from_db.copy()\n","\n","# 문자열 형식을 datetime형식으로 변환\n","df_work['date_rdate']= pd.to_datetime(df_work['rdate'])\n","\n","# 하루치 flatten된 뉴스 속보 저장을 위한 df 생성\n","df_work_result = pd.DataFrame()\n","\n","# 시작시간: 0시0분\n","c_time = pd.to_datetime(df_news_from_db.at[len(df_news_from_db)-1,'rdate'])\n","# 종료시간: 23시59분\n","e_time = pd.to_datetime(df_news_from_db.at[0,'rdate'])\n","\n","# 당일 0시0분 부터 23시59분까지 처리\n","while c_time <= e_time:\n","    # 1분씩 증가\n","    n_time = c_time +pd.Timedelta(minutes=1)\n","    # 실행 확인용 화면 출력\n","    #print('-',c_time, n_time)\n","    # 같은 시간대(분 단위)에 나온 속보 모두 추출\n","    df_sametime= df_work.loc[(df_work['date_rdate']>=c_time) & (df_work['date_rdate']<n_time),['date_rdate','20000_encoded_text_code']]\n","    # 뉴스 속보 벡터열 넣을 문자열 변수 생성, 초기화\n","    f_string=''\n","    # 같은 시간대 나온 모든 뉴스 속보 벡터를 하나로 합치기 flatten\n","    if len(df_sametime)>=1:     # 뉴스가 1개 이상 존재하는 경우\n","        for i in range(len(df_sametime)):   # 뉴스 갯수 만큼 반복\n","            # 실행 확인용 화면 출력\n","            #print('--',i)\n","            # 벡터 숫자만 꺼내기: 양 끝 기호 [ ] 없애고 중간에 있는 \\n삭제\n","            t_str = df_sametime.iloc[i]['20000_encoded_text_code'].strip('[]').replace('\\n','')\n","            # 앞 뉴스 벡터열에 이어서 붙이기, 공백 ' ' 을 사이에 두고\n","            f_string = f_string +' '+ t_str\n","            # 혹시 공백 여러개 있으면 하나로 정리해서 넣기\n","            f_string = re.sub(' +', ' ', f_string)\n","        # 같은 시간대 뉴스 벡터 모두 합쳤으면 앞 뒤로 [ ] 기호 붙이기\n","        f_string = '['+f_string+']'\n","        # 실행 확인용 화면 출력\n","        #print(f_string)\n","    else:   # 뉴스가 없는 시간대(분 단위)의 경우\n","        # 속보 벡터열을 빈 문자열로 처리\n","        f_string=''\n","    # 저장을 위한 임시 df 생성, 초기화: 현재 시간, 뉴스 갯수, 벡터 스트링\n","    temp_df = pd.DataFrame({'rdate':[c_time],'news_cnt':[len(df_sametime)],'2000_encoded_text_code':[f_string]})\n","    # 하루치 flatten된 뉴스 속보 저장을 위한 결과에 추가\n","    df_work_result= pd.concat([df_work_result,temp_df], ignore_index=True)\n","    # 다음 시간(분단위)을 현재 시간으로 설정: 반복 위해서\n","    c_time=n_time\n","#----------------------------------------------------------------"],"metadata":{"id":"QOJB0IdNPoYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행 확인을 위한 화면 출력\n","df_work_result.tail()"],"metadata":{"id":"FiHbzOcEQnx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--------------------------------------------------------\n","# Flatten 결과 DB에 저장하기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 데이터프레임을 SQLite 테이블로 저장\n","df_work_result.to_sql('news_4train_flatten', conn, if_exists='replace', index=False)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#--------------------------------------------------------\n","\n","#--------------------------------------------------------\n","# Flatten 결과 DB에서 가져오기\n","# 뉴스를 DB에서 꺼내오기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 읽어오는 SQL\n","query = \"SELECT * FROM news_4train_flatten\"\n","# sql 실행 결과 데이터프레임으로 받기\n","df_news_from_db = pd.read_sql_query(query, conn)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#--------------------------------------------------------"],"metadata":{"id":"X2EpR92JRlSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DB 연결 준비\n","conn = sqlite3.connect('nlp_sample.db')\n","# 종목 지정:  stock_code 041190\n","scode = '005930'\n","# DB에서 종목 데이터  추출위한 sql 준비\n","sql = f\"SELECT * FROM A{scode}_bun_label \"\n","# DB 검색결과를 dataframe에 저장\n","stock_df = pd.read_sql_query(sql, conn)\n","# DB close\n","conn.close()\n","\n","# 실행확인을 위한 화면 출력\n","print('주가 시작:',stock_df.at[0,'date'])\n","print('주가   끝:',stock_df.at[len(stock_df)-1,'date'])\n","\n","print('뉴스 시작:',df_news_from_db.at[len(df_news_from_db)-1,'rdate'])\n","print('뉴스   끝:',df_news_from_db.at[0,'rdate'])\n"],"metadata":{"id":"HL8xwHDAq8Zh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 안전한 날짜 연산을 위해서 datetime 형태의 날짜 컬럼을 만든다\n","stock_df['tdate'] = pd.to_datetime(stock_df['date'].astype(str))"],"metadata":{"id":"t1sTMmhJ1Ut0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행확인을 위한 화면 출력\n","stock_df.head()"],"metadata":{"id":"Udls4XBOIKiE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 주가 데이터 길이\n","len(stock_df)"],"metadata":{"id":"ogM8c2hWL0he","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710729233391,"user_tz":-540,"elapsed":405,"user":{"displayName":"hyun","userId":"14298231370728061195"}},"outputId":"4cde159d-73c1-4fe5-e14a-59cb39297064"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["190702"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["# 뉴스 당일과 그 다음날 주가 정보만 잘라두기\n","stock_df = stock_df[(stock_df['tdate'].dt.date >= pd.to_datetime(df_news_from_db.at[len(df_news_from_db)-1,'rdate']).date())\n","                    &(stock_df['tdate'].dt.date <= pd.to_datetime(df_news_from_db.at[len(df_news_from_db)-1,'rdate']).date()+pd.Timedelta(days=1))\n","                    ]"],"metadata":{"id":"ClArPV6C15Hq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 뉴스 당일과 그 다음날 주가 정보 갯수\n","len(stock_df)"],"metadata":{"id":"WvCdmLe3MVDt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710729264728,"user_tz":-540,"elapsed":440,"user":{"displayName":"hyun","userId":"14298231370728061195"}},"outputId":"35f85c5d-2211-408c-cdf7-af901c8605ef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["762"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["#----------------------------------------------------\n","#시작시간부터 끝 시간까지 1분 단위로 행 만들어서 주가 레이블 정보 끼워 넣기\n","stock_df.sort_values('tdate', ascending=False, inplace=True, ignore_index=True)\n","stock_df.head()\n","# df 복사\n","temp_df = stock_df.copy()\n","# 시작시간:\n","c_time= temp_df.iloc[0]['tdate']\n","# 종료시간:\n","e_time = temp_df.iloc[-1]['tdate']\n","\n","# 날짜 범위 생성\n","date_range = pd.date_range(start=e_time, end=c_time, freq='1min')\n","\n","# 데이터프레임 생성\n","left_temp_df = pd.DataFrame({'tdate': date_range})\n","# 역순 정렬\n","left_temp_df.sort_values('tdate', ascending=False, ignore_index=True, inplace=True)\n","\n","lj_result_df = pd.merge(left_temp_df, temp_df, on='tdate', how='left')\n","#print(lj_result_df.head(20))\n","# bun_rate와 bun_label이 Nan 인 경우 앞 시간의 값 rate, label을 넣는다\n","lj_result_df['bun_rate'] = lj_result_df['bun_rate'].fillna(method='ffill')\n","lj_result_df['bun_label'] = lj_result_df['bun_label'].fillna(method='ffill')\n","#print(lj_result_df.head(10))\n","stock_df=lj_result_df.copy()\n","#----------------------------------------------------\n","# 컬럼명 수정: 종목번호 추가\n","stock_df.rename(columns={'date':'005930_date','bun_rate': '005930_bun_rate','bun_label': '005930_bun_label'}, inplace=True)\n","\n","# 해당 날짜 뉴스 테이블 읽어오기\n","one_news_df= df_work_result.copy()\n","# one_news_df에 tdate 만들기, 20230102 날짜 하루 뉴스\n","one_news_df['tdate']=pd.to_datetime(one_news_df['rdate'])\n","# 뉴스 속보 테이블에 종목 분별 레이블 join 시켜서 넣기(left join)\n","left_join_result_df=pd.merge(one_news_df, stock_df, on='tdate', how='left')"],"metadata":{"id":"D2etwLp9q8XI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행 확인을 위한 화면 출력\n","left_join_result_df"],"metadata":{"id":"pelTuDTwUMfG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#====================================================================\n","# 데이터 셋으로 정리하기\n","df_dataset = left_join_result_df[['2000_encoded_text_code','005930_bun_label']]\n","df_dataset = df_dataset[df_dataset['2000_encoded_text_code'] != '']\n","df_dataset.columns=['text','label']\n","# 레이블 숫자로 변환 return\n","df_dataset['label'] = df_dataset['label'].replace({'U': 2, 'H': 1, 'D': 0})\n","#===================================================================="],"metadata":{"id":"q-DLhQGgXz_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행 확인을 위한 화면 출력\n","df_dataset"],"metadata":{"id":"GmH28iXBUMcw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#* 4. 예측 모델 만들기\n","- **LSTM**"],"metadata":{"id":"AGhiSgssZ56y"}},{"cell_type":"code","source":["#---------------------------------------------------------------\n","# 예측 모델 만들기: LSTM 카테고리 분류 모델\n","#-- import packages --------------------------------------------\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Embedding\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.datasets import reuters\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import matplotlib.pyplot as plt\n","#----------------------------------------------------------------\n","# 4.1 데이터셋 로딩\n","# data load: 데이터X,레이블y 분리\n","X = df_dataset['text']\n","y = df_dataset['label']\n","\n","# 안전한 데이터 로딩을 위해서 문자열 형식인 데이터X를 리스트로 확인 변환\n","X= X.apply(lambda x: [int(i) for i in x.strip('[]').split()])\n","\n","# 레이블이 1/0 아닌 경우: 원핫 인코딩 처리\n","y = to_categorical(y)\n","\n","# data set separation: 학습셋, 테스트셋 분리\n","X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.2, shuffle=True)\n","\n","# 레이블 카테고리 지정\n","category = 3\n","\n","# 실행 확인을 위한 화면 출력\n","print(category,'카테고리')\n","print(len(X_train),'학습용 뉴스 기사')\n","print(len(X_test), '테스트용 뉴스 기사')\n","\n","# pad_sequence 단어 수 맞춰주기: 한 기사당 단어 갯수 제한\n","sequence_length = 50\n","X_train = sequence.pad_sequences(X_train, maxlen= sequence_length)\n","X_test = sequence.pad_sequences(X_test,maxlen=sequence_length)\n","\n","####################################################################################\n","#-----------------------------------------------------------------------------------\n","# 4.2 모델 생성\n","# model creation: LSTM\n","model = Sequential()\n","model.add(Embedding(20000,sequence_length))         # Input : 임베딩(2만단어사전, 50단어)\n","model.add(LSTM(sequence_length, activation='tanh')) # LSTM  : 활성화: tanh\n","model.add(Dense(category, activation='softmax'))    # Output: ternary 이상인 경우\n","\n","#-------------------------------------------------------------\n","# 4.3 모델 컴파일\n","# model compile, 실행 옵션\n","model.compile(\n","    optimizer = 'adam'                    # 최적화 함수 : adam\n","  , loss = 'categorical_crossentropy'     # 손실 함수   : categorical_crossentropy(ternary 이상인 경우)\n","  , metrics = ['accuracy']                # 측정 지표   : 정확도 accuracy\n",")\n","\n","# 조기중단 설정, early stopping callback\n","early_stopping_callback = EarlyStopping(\n","    monitor= 'val_loss'                   # 모니터 항목 : val_loss\n","  , patience = 10                          # patiencd    : 5 (epochs)\n",")\n","\n","# 최적화 모델 저장, best model saving checkpoint\n","modelpath = f'./data/model/LSTM_news.hdf5' # 모델저장위치와 이름\n","\n","# checkpointer 생성\n","checkpointer = ModelCheckpoint(\n","    filepath = modelpath        # filepath\n","  , monitor = 'val_loss'        # 모니터  : val_loss\n","  , verbose = 0                 # verbose : 0\n","  , save_best_only = True       # save_best_only: True\n",")\n","#-------------------------------------------------------------\n","# 4.4 모델 핏\n","# model fit, 모델 학습 실행, history 저장\n","history = model.fit(\n","        X_train                     # 학습 데이터\n","    ,   y_train                     # 레이블\n","    ,   batch_size = 4              # batch_size  : 4\n","    ,   epochs = 10                 # epochs      : 10\n","    ,   validation_data = (X_test, y_test)    # validation_data\n","    ,   callbacks = [               # callbacks\n","            early_stopping_callback # 조기중단\n","          , checkpointer            # best model\n","        ]\n",")\n","\n","#-------------------------------------------------------------\n","# 4.5 모델 테스트\n","# 시각화\n","# 테스트 정확도 출력\n","print(f'\\n Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}')\n","\n","# 검증셋과 학습셋의 오차를 저장\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","# 그래프 그리기\n","# x축: 실행한 epochs 횟수\n","x_len = np.arange(len(y_loss))\n","# 선그래프: 테스트셋 손실, red\n","plt.plot(x_len, y_vloss, marker='.',c='red',label='Testset_loss')\n","# 선그래프: 학습셋 손실, blue\n","plt.plot(x_len, y_loss, marker='.',c='blue', label='Trainset_loss')\n","# 그래프 legend 추가, 위치 지정\n","plt.legend(loc='upper right')\n","# 그래프에 그리드 표시\n","plt.grid()\n","# x 축 레이블\n","plt.xlabel('epoch')\n","# y 축 레이블\n","plt.ylabel('loss')\n","# 그래프 화면에 출력\n","plt.show()\n","#--------------------------------------------------------------\n","# 예측모델 결과 저장 확인하기!\n","#=============================================================="],"metadata":{"id":"3eARr24EUMac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#5. 예측 하기\n","\n","\n","1. 예측 데이터\n","2. 예측 모델 load\n","3. 예측 실행\n","4. 예측 결과"],"metadata":{"id":"xFnEitpX3m0W"}},{"cell_type":"code","source":["#----------------------------------------------------------------\n","# 예측 데이터 가져오기\n","# 데이터 전처리\n","# 모델 입력 데이터 셋 생성\n","# 1.웹 크롤링: 오늘 날짜 뉴스 가져다가 예측(분류) 해보기!\n","news_url= f\"https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258\"\n","# 웹 request 위한 html 형식 설정\n","html = requests.get(news_url)\n","# BeautifulSoup 으로 해당 웹페이지 읽어오기\n","soup = BeautifulSoup(html.text, \"lxml\")\n","# <dd class='articleSummary'> 태그 목록 만들기\n","ddsum_list = soup.find_all('dd', attrs={\"class\": \"articleSummary\"})\n","# 태그들을 '^'구분해서 분리\n","ddsum_test_list = ddsum_list[0].getText('^').strip().split('^')\n","# 2줄 요약 news_summary 문자열 추출\n","news_summary  = ddsum_test_list[0].strip()\n","# 신문사 press_name 문자열 추출\n","press_name    = ddsum_test_list[1].strip()\n","# 배포날짜 release_date 문자열 추출\n","release_date  = ddsum_test_list[5].strip()\n","# 추출한 위 3 항목을 DataFrame으로 저장\n","temp_df = pd.DataFrame(data = [[news_summary,press_name,release_date]], columns=['summary','press','rdate'])\n","# 실행 확인용 화면 출력\n","print('오늘: summary',temp_df['summary'].values)\n","print('오늘: press',temp_df['press'].values)\n","print('오늘: rdate',temp_df['rdate'].values)\n","# 현재 페이지 전체 뉴스 저장을 위한 news_df 생성\n","news_df = pd.DataFrame()\n","# 뉴스 있는 동안 반복\n","if len(ddsum_list)>0:\n","  # <dd> 태그 목록에서 하나씩 꺼내기\n","  for ddsum in ddsum_list:\n","    # <dd> 와 </dd> 사이의 텍스트를 추출, 다른 태그 있을시 '^'구분해서 추출, 앞뒤 공백제거\n","    data_string = ddsum.getText('^').strip()\n","    # 추출한 문자열을 '^' 경계로 분리\n","    string_list = data_string.split('^')\n","    # 2줄 요약 news_summary 문자열 추출\n","    news_summary  = string_list[0].strip()\n","    # 신문사 press_name 문자열 추출\n","    press_name    = string_list[1].strip()\n","    # 배포날짜 release_date 문자열 추출\n","    release_date  = string_list[5].strip()\n","    # 추출한 위 3 항목을 DataFrame으로 저장\n","    temp_df = pd.DataFrame(data = [[news_summary,press_name,release_date]], columns=['summary','press','rdate'])\n","    # 현재 페이지 전체 뉴스를 위한 news_df에 추가\n","    news_df = pd.concat([news_df, temp_df], ignore_index=True)\n","#==================================================================\n","# 뉴스를 DB에 저장하기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 데이터프레임을 SQLite 테이블로 저장\n","news_df.to_sql('news_today', conn, if_exists='replace', index=False)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#==================================================================\n","#==================================================================\n","# 뉴스를 DB에서 꺼내오기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 읽어오는 SQL\n","query = \"SELECT * FROM news_today\"\n","# sql 실행 결과 데이터프레임으로 받기\n","df_news_today = pd.read_sql_query(query, conn)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#==================================================================\n","# 뉴스토큰화\n","#-------------------------------------------------------------------\n","# 토큰화를 위한 konlpy의 Okt 객체 생성\n","okt=Okt()\n","# okt로 tokenize해서 형태소pos 추출하기 (tokenize 전체는 okt.morphs, 명사는 okt.nouns)\n","df_news_today['morphs_tokenized_summary'] = df_news_today['summary'].apply(okt.morphs)\n","# DB 저장을 위해 문자열로 변환\n","df_news_today['morphs_tokenized_summary'] = df_news_today['morphs_tokenized_summary'].astype(str)\n","# okt로 tokenize해서 형태소pos 추출하기 (tokenize 전체는 okt.morphs, 명사는 okt.nouns)\n","df_news_today['pos_tokenized_summary'] = df_news_today['summary'].apply(okt.pos)\n","# DB 저장을 위해 문자열로 변환\n","df_news_today['pos_tokenized_summary'] = df_news_today['pos_tokenized_summary'].astype(str)\n","# okt로 tokenize해서 형태소pos 추출하기 (tokenize 전체는 okt.morphs, 명사는 okt.nouns)\n","df_news_today['nouns_tokenized_summary'] = df_news_today['summary'].apply(okt.nouns)\n","# DB 저장을 위해 문자열로 변환\n","df_news_today['nouns_tokenized_summary'] = df_news_today['nouns_tokenized_summary'].astype(str)\n","# pos tokenized 에서 Modifier,Josa,Suffix,Punctuation,Foreigh 빼기, 1 자리 단어 빼기\n","df_news_today['tokenized_summary'] = \"\"\n","# 모든 뉴스처리\n","for i in range(len(df_news_today)):\n","  # 토큰화된 뉴스 임시 저장\n","  new_str = []\n","  # 형태소 분리된 내용을 튜플로 꺼내기\n","  for (x,y) in ast.literal_eval(df_news_today['pos_tokenized_summary'][i]):\n","    #관형사, 조사, 접두어, 문장부호, 한자등 제외\n","    if (y!='Modifier')&(y!='Josa')&(y!='Suffix')&(y!='Punctuation')&(y!='Foreign'):\n","      # 1자리 단어도 제외\n","      if(len(str(x))>1):\n","        # 남은 것 모두 붙이기\n","        new_str.append(x)\n","  #토큰화 결과 컬럼에 넣기\n","  df_news_today['tokenized_summary'][i]=str(new_str)\n","#-------------------------------------------------------------------\n","#===================================================================\n","# 커스터마이즈된 TextVectorization 객체 생성\n","#-------------------------------------------------------------------\n","# custom_표준화 함수\n","def custom_standardization_fn(string_tensor):\n","    # 한글의 경우 소문자 처리 안해도 됨\n","    lowercase_string = string_tensor\n","    # 문장부호중에 '%','.' 는 남기기\n","    return tf.strings.regex_replace(\n","        lowercase_string, f\"[{re.escape(string.punctuation.replace('%','').replace('.',''))}]\", \"\")\n","\n","# custom_토큰화 함수\n","def custom_split_fn(string_tensor):\n","    # 한글의 경우 형태소 분석 처리 해야 한다: 우린 이미 okt로 했음\n","    return tf.strings.split(string_tensor)\n","\n","# custom 표준화, 토큰화 함수 지정해서 TextVectoriztion 생성하기\n","text_vectorization = TextVectorization(\n","    #--- max_tokens 지정--\n","    max_tokens=20000\n","    #---------------------\n","    # 정수 인덱스로 출력\n","  ,  output_mode=\"int\"\n","    # 표준화 함수를 custom_표준화 함수로 지정\n","  ,  standardize=custom_standardization_fn\n","    # 토큰화 함수를 custom_토큰화 함수로 지정\n","  ,  split=custom_split_fn\n",")\n","# =========================================================\n","# 단어사전 불러오기(이미 만들어진 사전)\n","# --- 불러오기 load : start ---\n","from_disk = pickle.load(open(\"tv_2man.pkl\", \"rb\"))\n","\n","new_vectorizer = TextVectorization(\n","    # 정수 인덱스로 출력\n","    output_mode=\"int\",\n","    # 표준화 함수를 custom_표준화 함수로 지정\n","    standardize=custom_standardization_fn,\n","    # 토큰화 함수를 custom_토큰화 함수로 지정\n","    split=custom_split_fn,\n","  )\n","new_vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n","new_vectorizer.set_weights(from_disk['weights'])\n","# --- 불러오기 load : end ---\n","# =============================================================\n","\n","# =============================================================\n","# 뉴스 벡터화 하기\n","# 토큰화된 것을 사전이용해서 벡터화 하고 텐서안의 인덱스 리스트를 꺼내서 따로 저장\n","for i in range(len(df_news_today)):\n","  df_news_today.at[i,'20000_encoded_text_code']= str(new_vectorizer(df_news_today.at[i,'tokenized_summary']).numpy()).replace('\\n', '')\n","# =============================================================\n","# 실행 확인용 화면 출력\n","print(df_news_today.head())\n","\n","# 예측 데이터 X_test 생성\n","X = df_news_today['20000_encoded_text_code']\n","# 벡터 리스트형태도 다시 확인 생성\n","X= X.apply(lambda x: [int(i) for i in x.strip('[]').split()])\n","# LSTM 모델 입력을 위한 패딩 맞추기\n","sequence_length = 50\n","# 입력을 위한 최종 sequence 형태 맞추기\n","X = sequence.pad_sequences(X, maxlen= sequence_length)\n","#-------------------------------------------------------------------\n","# 예측 데이터 처리 끝\n","#==================================================================="],"metadata":{"id":"zZoxFM-qZ-Wg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#======================================================================\n","# 예측 모델 로딩, 실행, 결과\n","#------------------------------------------------------------------------------\n","from tensorflow.keras.models import Model, Sequential,load_model\n","\n","# 예측 모델 load\n","model = load_model(filepath=f'/content/data/model/LSTM_news.hdf5')\n","# 예측 실행\n","stock_outlook = np.argmax(model.predict(X), axis=1)\n","# 예측 결과 저장\n","df_news_today['outlook'] = stock_outlook\n","#-----------------------------------------------------------------------------------"],"metadata":{"id":"n4VEHqgTUMX8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710729756807,"user_tz":-540,"elapsed":1678,"user":{"displayName":"hyun","userId":"14298231370728061195"}},"outputId":"e0c20d36-7356-45f5-c3b3-9fe03531540c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 585ms/step\n"]}]},{"cell_type":"code","source":["# 실행 확인을 위한 화면 출력\n","df_news_today[['summary','outlook']]"],"metadata":{"id":"BlBN5xFGUMVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--------------------------------------------------------\n","# 예측 결과 DB에 저장\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 데이터프레임을 SQLite 테이블로 저장\n","df_news_today.to_sql('news_today_outlook', conn, if_exists='replace', index=False)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#---------------------------------------------------------\n","#---------------------------------------------------------\n","# 예측 결과 DB에서 꺼내오기\n","# DB 커넥션 생성\n","conn = sqlite3.connect('nlp_sample.db')\n","# 읽어오는 SQL\n","query = \"SELECT * FROM news_today_outlook\"\n","# sql 실행 결과 데이터프레임으로 받기\n","df_news_today_outlook = pd.read_sql_query(query, conn)\n","# 데이터베이스 연결 종료\n","conn.close()\n","#---------------------------------------------------------"],"metadata":{"id":"HOxNbrKYFEvv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행 확인용 화면 출력\n","print(df_news_today_outlook.head())"],"metadata":{"id":"76D2qO9DF8qY"},"execution_count":null,"outputs":[]}]}